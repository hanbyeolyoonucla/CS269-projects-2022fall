---
layout: post
comments: true
title: RL-based Trajectory Planning for Dental Surgery
author: Hanbyeol Yoon (Team 03)
date: 2022-09-19
---

<!--
ABSTRACT
1. In tooth prep in dental surgery, it is important to acheive accuracy and speed.
2. However, dentists have done manually but the robot arm has to be commanded manually all trajectories.
3. In this paper, we would like to propose rl-based planner for dental surgery specifically tooth prep for crown
4. To automate this procedure with rl agent in simulator and impose the traj in real world to make to robot arm follow
5. how: RoboDK with robot and tool info + openAI gym for trial and error to find the best traj while avoiding various types of obstaacles in patient jaw. 
6. brief result - expected result: we will simulate and visualize in the simulation that the trajectory has been generated in 
engineer-manually hard coded trajectory 
evaluate time, accuracy and safety(obstacale avoidance etc)
hopefully run in the real robot arm for evaluation as well
Keyword, path planning, obstacle avoidance, dental surgical robotics, reinforcement learning
-->
> With advancement in robotic technologies, in the field of medical surgery, artificial intelligence (AI) and machine learning (ML) plays a pivotal role in medical imaging, prognosis/diagnosis, treatment assistant, and automation of repetitive subtasks of surgery. However, specifically in dental surgery, reinforcement learning (RL) has not been actively applied while it has a lot in common with robot machining where RL is widely used for fully-automated trajectory generation. Therefore, in this article, by developing an RL-based tool trajectory planner, we would like to enable robot arm to automatically perform a few subtasks in dental surgery such as tooth preparation for crown through removal of cavity. We will use CopelliaSim for simulation environments, Python/Pytorch for RL, and MATLAB for robotics. The expected result of this project would be a RL-based, collision-free, fully-automated dental tool trajectory generation algorithm.

<!--more-->
{: class="table-of-content"}
* [Introduction](#introduction)
* [Background](#background)
    * [Tooth Preparation for Crown](#tooth-preparation-for-crown)
    * [Kinematics of Robotics](#kinematics-of-robotics)
    * [Reinforcement Learning](#reinforcement-learning)
* [Methodology](#methodology)
    * [Interaction Environment](#interaction-environment)
    * [Reward Function](#reward-function)
    * [Training Process](#training-process)
* [Results](#results)
    * [Experimental Setup](#experimental-setup)
    * [Experimental Results](#experimental-results)
    * [Analysis and Evaluation](#analysis-and-evaluation)
* [Conclusions](#conclusion)
* [References](#references)

## Introduction

With the advent of an aging society and the advancement in robotic technologies, dentistry industry becomes one of the biggest markets in the field of robotics. Since various surgeries in dentistry involve high-precision and repetitive tasks, robot technologies have significant potential and ability to assist dentists or perform certain types of tasks over human practitioners. Many researches have been conducted applying the robot technologies to dentistry such as maxillofacial surgery [[1-3]](#reference), tooth preparation [[4]](#reference), and robot assistant [[5]](#reference).

On top of that, along with the robotics technologies, machine learning (ML) and artificial intelligence (AI) has been used in dentistry as well. The ML and AI is widely exploited in dental imaging and diagnosis: clinical diagnosis [[6]](#reference), dental radiography [[8]](#reference), and oral cancer prediction [[9]](#reference). However, compared to other fields of surgical robots [[10-14]](#reference), reinforcement learning (RL) has not been actively applied in dentistry. For the most commonly used commercial surgical robot, da Vinci System, an open-source environment is provided for users to apply reinforcement learning algorithms for controlling the surgical arms [[10-11]](#reference). While subtasks of surgery in dentistry such as implantology and tooth preparation for crown has a lot in common with the field of robot machining where RL also has been actively studied and applied, [[15]](#reference) to the best of my knowledge, there are not many researches being conducted in RL-based dental robot control.


![dentronics]({{ '/assets/images/team03/dentronics.jpg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. Vision of possible robots and artificial intelligence service network to support future dentistry [[16]](#reference).


In this regard, in this article, we would like to take advantage of RL to automate subtasks in dental surgery. Specifically, we will propose an rl-based trajectory planner for a robot arm to accomplish automated tooth preparation for crown. The expected contribution of our project is summarized as follow:
- Reinforcement Learning environment for dental surgical robot
- 3D CAD model based fully-automatic trajectory generation
- Guarantee of collision-free trajectory with appropriately defined reward function and collision-check in simulation

The structure of the rest of the article is as follow: 1) background for tooth preparation for crown, kinematics of robotics, and reinforcement learning, 2) methodology we use for this project, 3) result of the simulation of rl-based trajectory planner, and 4) conclusion.

## Background
### Tooth Preparation for Crown
A dental crown is a cap which is used to cover teeth as shown in figure 2. In this project, a robot arm will be controlled to remove decayed areas and smooth the surfaces of the tooth before crowning. In order to automatically generate robot tool trajectory, reinforcement learning will be implemented. To briefly explain how reward is defined, 3D CAD file of decayed areas is assumed to be available in the form of stl file which can be converted into point clouds. Based on the distance between each points in the point clouds and dental tool contact points, the points will be eliminated, which means the decayed areas being removed. After a few episodes, we expect the agent (robot arm) to learn how to behave in the environment where the less points are left, the more rewards it receives. In the end, the robot arm will be controlled in such a way that the decayed areas are removed by collion-free trajectory of the robot arm. The RL-based control algorithm will be evaluated by how robustly and precisely the end effector removes decayed areas while avoiding obstacles inside patient's jaw.


![crownprep]({{ '/assets/images/team03/dental_crown_prep.jpg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2. Tooth Preparation Procedure for Crown. (Image source: <https://www.dndentalcare.com/crowns.html>)*


### Kinematics of Robotics
explanation on kinematics of robotics e.g. how tool trajectory is converted into a set of joint angles of a robot arm.

![YOMI]({{ '/assets/images/team03/robot_assisted_implants_yomi.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 3. Yomi: commercial implantology robot arm. (Image source: <https://www.neocis.com>)*


### Reinforcement Learning
explanation on our method of reinforment learning: MDP, DQN, PPO


## Methodology

### Interaction Environment
We use RoboDK, Python for this project. For robot manipulator, Meca 500 with 6 degrees of freedom will be used, forward and inverse kinematics of which is available implicitly in RoboDK. The end effector which is developed by Cyberdontics is attached at the robot flange, the end tip of which is used for removing decayed areas. The 3D model of patient's jaw is available and loaded in RoboDK to visualize and evaluate the process of crown preparation.


![environment]({{ '/assets/images/team03/simulationenv.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 4. The overview of the simulation environment.* [[15]](#reference)

### Action Space
The action is a `ndarray` with shape `(6,)` which can take values between Min and Max indicating the increment of each joint angle in consideration of Min and Max joint velocity of the robot manipulator .
| Num | Observation    | Min                 | Max               |
|-----|----------------|---------------------|-------------------|
| 0   | Joint1 Angle   | -1.5°               | 1.5°              |
| 1   | Joint2 Angle   | -1.5°                | 1.5°               |
| 2   | Joint3 Angle   | -1.8°               | 1.8°               |
| 3   | Joint4 Angle   | -3°               | 3°              |
| 4   | Joint5 Angle   | -3°               | 3°              |
| 5   | Joint6 Angle   | -5°               | 5°              |

**Note**: The range of joint angle increment is available in the following link: <https://support.mecademic.com/support/home>

### Observation Space
The observation is a `ndarray` with shape `(6,)` with the values corresponding to the joint angles of robot manipulator:
| Num | Observation    | Min                 | Max               |
|-----|----------------|---------------------|-------------------|
| 0   | Joint1 Angle   | -175°               | 175°              |
| 1   | Joint2 Angle   | -70°                | 90°               |
| 2   | Joint3 Angle   | -135°               | 70°               |
| 3   | Joint4 Angle   | -170°               | 170°              |
| 4   | Joint5 Angle   | -115°               | 115°              |
| 5   | Joint6 Angle   | -180°               | 180°              |

**Note**: The range of joint angles is available in the following link: <https://support.mecademic.com/support/home>

### Rewards
Since the goal is to removed decayed areas which is represented by point clouds with minimal time, a reward of `+1` for every points removed and a reward of `-1` for every time step, including the termination step, is allotted.

### Starting State
All observations are assigned zero to account for homing position of the robot manipulator.

### Episode End
The episode ends if any one of the following occurs:
1. Termination: There exist any collision between robot arm and jaw
2. Truncation: Episode length is greater than 500

### Arguments
```
gym.make('gym_examples/DentalSurgery-v0')
```

### Baselines PPO Implementation

```
from robodk import robolink    # RoboDK API
from robodk import robomath    # Robot toolbox
import numpy as np
from stl import mesh
import gym
import gym_examples
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env

# Press the green button in the gutter to run the script.
if __name__ == '__main__':

    RDK = robolink.Robolink()
    env = gym.make('gym_examples/DentalSurgery-v0')
    # check_env(env)

    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10_000)

    obs, _ = env.reset()
    for i in range(1000):
        print(i)
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = env.step(action)
        if done:
            obs = env.reset()

    env.close()
```

## Results
Simulation of trajectory planner and evaluatation of the result by comparing it with the desired 3D CAD model.
### Experimental Setup
### Experimental Results
### Analysis and Evaluation

## Conclusion

## Reference
<!--maxillofacial surgery-->
[1] Sun X, McKenzie FD, Bawab S, Li J, Yoon Y, Huang JK. "Automated dental implantation using image-guidedrobotics: registration results." Int J Comput Assist RadiolSurg 2011;6:627–34.

[2] Zhenggang C, Qin C, Fan S, Yu D, Wu Y, Chen X. "Pilot studyof a surgical robot system for zygomatic implantplacement." Med Eng Phys 2019.

[3] Woo S-Y, Lee S-J, Yoo J-Y, Han J-J, Hwang S-J, Huh K-H, et al. "Autonomous bone reposition around anatomical landmarkfor robot-assisted orthognathic surgery." J Cranio-MaxillofacSurg 2017;45:1980–8.

<!--tooth prep-->
[4] Otani T, Raigrodski A, Mancl L, Kanuma I, Rosen J. "In vitroevaluation of accuracy and precision of automated robotictooth preparation system for porcelain laminate veneers." JProsthetic Dent 2015:114.

[5] Yuan, Fusong, and Peijun Lyu. "A preliminary study on a tooth preparation robot." Advances in Applied Ceramics 119.5-6 (2020): 332-337.

<!--robot assistant-->
[6] Grischke J, Johannsmeier L, Eich L, Haddadin S. "Dentronics:first concepts and pilot study of a new application domainfor collaborative robots in dental assistance." In: 2019 international conference on robotics and automation(ICRA). 2019.

<!--radiography-->
<!--dental imaging [6], radiography [7], prediction of oral treatment [8], prognosisof oral cancer [9], etc.-->
[7] Chan Y-K, Chen Y-F, Pham T, Chang W, Hsieh M-Y. Artificialintelligence in medical applications. J Healthcare Eng2018;2018:2.

[8] Chan Y-K, Chen Y-F, Pham T, Chang W, Hsieh M-Y. Artificialintelligence in medical applications. J Healthcare Eng2018;2018:2.

[9] Wang X, Yang J, Wei C, Zhou G, Wu L, Gao Q, et al. Apersonalized computational model predicts cancer risklevel of oral potentially malignant disorders and its webapplication for promotion of non-invasive screening. J OralPathol Med 2019.

<!--rl surgical robot-->
<!--rl opensource environment-->
[10] Richter, Florian, Ryan K. Orosco, and Michael C. Yip. "Open-sourced reinforcement learning environments for surgical robotics." arXiv preprint arXiv:1903.02090 (2019).

[11] Xu, Jiaqi, et al. "Surrol: An open-source reinforcement learning centered and dvrk compatible platform for surgical robot learning." 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2021.

<!--rl path planning for suturing and tissue manipulation-->
[12] Baek, Donghoon, et al. "Path planning for automation of surgery robot based on probabilistic roadmap and reinforcement learning." 2018 15th International Conference on Ubiquitous Robots (UR). IEEE, 2018.

[13] Varier, Vignesh Manoj, et al. "Collaborative suturing: A reinforcement learning approach to automate hand-off task in suturing for surgical robots." 2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN). IEEE, 2020.

[14] Shin, Changyeob, et al. "Autonomous tissue manipulation via surgical robot using learning based model predictive control." 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.

<!--rl robot machining-->
[15] Zhong, Jie, Tao Wang, and Lianglun Cheng. "Collision-free path planning for welding manipulator via hybrid algorithm of deep reinforcement learning and inverse kinematics." Complex & Intelligent Systems 8.3 (2022): 1899-1912.

[16] Grischke, Jasmin, et al. "Dentronics: Towards robotics and artificial intelligence in dentistry." Dental Materials 36.6 (2020): 765-778.

---

<!--
## Data Rich and Physics Certain

| Experiment 					| Parameters  											| Results  								| Comments 							|
| :---       					|    :----:   											|     :---: 							|     ---: 							|
| **DL + Data**																																						|

| Predicting only velocity  	| Dataset size : 10000<br> Network : 2->5->5->1 <br> activation: ReLU	|  ~100% accurate	| Generalises well over various initial velocities |
| Predicting only displacement 	| Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable		| Better prediction for $u_0 \in dataset$, average prediction outside | 
| Predicting both $v_t, s_t$	| Dataset size : 10000<br> Network : 2->16->16->2 <br>	activation: tanh	|	Reasonable		| Better prediction for $u_0 \in dataset$, poor prediction outside |

-----

| **DL + Physics**																																			|
| Predicting both $v_t, s_t$, using Loss $L_{physics} = \|v_{predicted}^2-u_{initial}^2-2*g*s_{predicted}\|$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	~0% accuracy		| Expected result as no supervision of any kind is provided |
| Predicting both $v_t, s_t$, using Loss $L_{velocity+phy} = (v_{predicted}-v_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Prediction of $v_t$ is good. Was able to learn $s_t$ reasonably well without direct supervision |
| Predicting both $v_t, s_t$, using Loss $L_{supervised+phy} = (v_{predicted}-v_{actual})^2+(s_{predicted}-s_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Not a better result w.r.t direct supervision |


**Observations :** 
- Physics equations are certain in this case and are the best to use.
- Both DL, Hybrid(DL+Physics) methods performance are equivalent (actual accuracy/loss varies based on fine training, random dataset generation)

Re running the above experiments with Dataset size of 200(Data Starvation), yielded the following observations
- DL performance is comparable with 10000 dataset when trained on much mode epochs(5x)
- Hybrid(DL+Physics) without direct supervision on $s_t$ has comparable/better closeness than DL only method for limited epochs($\sim$300) training.




## Data Rich and Physics Uncertain

| Experiment 					| Parameters  											| Results  								| Comments 							|
| :---       					|    :----:   											|     :---: 							|     ---: 							|
| **DL + Data**																																						|\
| Predicting both $v_t, s_t$	| Dataset size : 10000<br> Network : 2->16->16->2 <br>	activation: tanh	|	Reasonable		| Better prediction for $u_0 \in dataset$, poor prediction outside |
| **DL + Physics**																																			|
| Predicting both $v_t, s_t$<br> using Loss $L_{physics} = \|v_{predicted}^2-u_{initial}^2-2*g*s_{predicted}\|$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	~0% accuracy		| Expected result as no supervision of any kind is provided |
| Predicting both $v_t, s_t$<br> using Loss $L_{velocity+phy} = (v_{predicted}-v_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Prediction of $v_t$ is good. Was able to learn $s_t$ reasonably well without direct supervision |
| Predicting both $v_t, s_t$<br> using Loss $L_{supervised+phy} = (v_{predicted}-v_{actual})^2+(s_{predicted}-s_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Not a better result w.r.t direct supervision, but bettr than DL when $u0$ is out of dataset |


**Observations :** 
- Both DL, Hybrid(DL+Physics) methods performance are similar, Hybrid(DL+Physics) is better when $u0$ is out of dataset, DL is better for $u0$ in dataset.
- Physics equations are not certain in this case and the above methods are better to use than Physics.

## Data Starvation and Physics Uncertain
- Similar observations as in data rich
-->

